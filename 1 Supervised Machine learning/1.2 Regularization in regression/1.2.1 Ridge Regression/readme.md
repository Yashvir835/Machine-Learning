# ğŸ“˜ Ridge Regression

---

## ğŸ“Œ Key Concepts

1. **Ridge Regression is also known as Tikhonov Regularization.**
2. The penalty (regularization) term is **only applied during training**, not during testing or validation.
3. Regularization helps in:
   - Controlling **overfitting** (high variance)
   - Handling **multicollinearity**  
     - When a predictor variable in a multiple regression model can be linearly predicted from the others

---

## âœ… Advantages of Ridge Regression

- Handles **overfitting** by penalizing large weights.
- Addresses **multicollinearity** effectively.
- Retains all variables, which is useful when every feature is somewhat informative.

---

## âŒ Disadvantages of Ridge Regression

- **Does not perform feature selection** â€” all features remain in the model, even if some are redundant or less impactful.
- Can introduce **bias** by shrinking coefficient values towards zero.

---

## ğŸ§® Cost Function

The regularized cost function in Ridge Regression is:

```text
J(w, b) = (1 / 2m) * Î£áµáµ¢=â‚ [fâ‚w,bâ‚(xâ½â±â¾) - yâ½â±â¾]Â² + (Î» / 2m) * Î£â¿â±¼=â‚wâ±¼Â²
```                                                               

Where:
- `m`: Number of training examples  
- `n`: Number of features (coefficients)  
- `Î»`: Regularization parameter (also denoted as `Î±`)  
- `wâ±¼`: Weight/parameter for feature `j`

---

## ğŸ“ Regularization Term

This is equivalent to the **squared L2 norm**:

```text
Regularization Term = (Î» / 2m) * ||w||Â² = (Î» / 2m) * Î£â¿â±¼=â‚ wâ±¼Â²
```

---

## ğŸ” Mathematical Intuition

Ridge Regression adds a constraint on the size of the weight vector `w`:

```text
Î£â¿â±¼=â‚ wâ±¼Â² â‰¤ t
```

### Geometrically:
![Ridge Plot](../lasso_ridge.jpg)

- In 2D:

  ```text
  wâ‚Â² + wâ‚‚Â² â‰¤ t
  ```

- In higher dimensions, this becomes a **hypersphere**.

- The threshold `t` is a function of `Î»`:  
  **Larger Î» â†’ smaller t â†’ stronger regularization**

---

## ğŸ§­ Visual Intuition: Loss Surface & Constraint

- The **squared loss** `(yáµ¢ - Å·áµ¢)Â²` forms **elliptical contour curves** in `w`-space (weight space).
- The **regularization constraint** `||w||Â² â‰¤ t` defines a **circular region** centered at the origin.

**Optimization Interpretation:**

1. Imagine sliding the ellipse (loss contour) toward the origin to minimize the loss.
2. The **moment this ellipse first touches the constraint region**, that point is the **optimal `w`**.

---

## ğŸ“ Summary

- Ridge regression is ideal when we want to **retain all features** but avoid overfitting.
- It **shrinks coefficients** rather than eliminating them.
- Useful in high-dimensional data and scenarios with **multicollinearity**.

---

